# LLM Environment variables

# Reasoning LLM (for complex reasoning tasks)
# If you're using your local Ollama, replace the model name after the slash and base url then you're good to go.
# For wider model support, read https://docs.litellm.ai/docs/providers.
REASONING_API_KEY=
REASONING_BASE_URL=http://localhost:11434
REASONING_MODEL=ollama_chat/deepseek-r1:7b

# Non-reasoning LLM (for straightforward tasks)
BASIC_API_KEY=sk-xxx
BASIC_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1
BASIC_MODEL=qwen-max-latest

# Vision-language LLM (for tasks requiring visual understanding)
VL_API_KEY=sk-xxx
VL_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1
VL_MODEL=qwen2.5-vl-72b-instruct

# Application Settings
DEBUG=True
APP_ENV=development

# Add other environment variables as needed
TAVILY_API_KEY=tvly-xxx
# CHROME_INSTANCE_PATH=/Applications/Google Chrome.app/Contents/MacOS/Google Chrome

# # AZURE LLM Config
# AZURE_API_BASE=https://xxxx
# AZURE_API_KEY=xxxxx
# AZURE_API_VERSION=2023-07-01-preview
# # Reasoning LLM (for complex reasoning tasks)
# REASONING_AZURE_DEPLOYMENT=xxx
# # Non-reasoning LLM (for straightforward tasks)
# BASIC_AZURE_DEPLOYMENT=gpt-4o-2024-08-06
# # Vision-language LLM (for tasks requiring visual understanding)
# VL_AZURE_DEPLOYMENT=gpt-4o-2024-08-06
